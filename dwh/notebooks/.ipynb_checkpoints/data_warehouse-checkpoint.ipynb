{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Warehouse Citibike"
   ],
   "metadata": {
    "id": "RjreYTH_PbTD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Project Info\n",
    "\n",
    "Group: **Real estate**\n",
    "\n",
    "Team Members: **Altin Kelmendi, Julian Hoffmann, Daniel Dodmasej, Clemens Hohensinner**\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "D6DgCqX1JPlF"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Setup &  Imports",
   "metadata": {
    "id": "RGSMZ8IDJuny"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, String, TIMESTAMP, Interval, ForeignKey, MetaData\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker\n",
    "\n",
    "# Database connection string (update as needed)\n",
    "DATABASE_URL = 'postgresql://root:root@localhost:5452/citibike_db'\n",
    "\n",
    "# SQLAlchemy setup\n",
    "engine = create_engine(DATABASE_URL)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "Base = declarative_base()\n",
    "metadata = MetaData()\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MdtMX-MWJtt2",
    "outputId": "7caae082-4205-46e0-a886-1500cd1c4571",
    "ExecuteTime": {
     "end_time": "2025-03-21T10:42:34.739641Z",
     "start_time": "2025-03-21T10:42:34.594146Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'psycopg2'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m DATABASE_URL \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpostgresql://root:root@localhost:5452/citibike_db\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# SQLAlchemy setup\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m engine \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDATABASE_URL\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m Session \u001B[38;5;241m=\u001B[39m sessionmaker(bind\u001B[38;5;241m=\u001B[39mengine)\n\u001B[1;32m     10\u001B[0m session \u001B[38;5;241m=\u001B[39m Session()\n",
      "File \u001B[0;32m<string>:2\u001B[0m, in \u001B[0;36mcreate_engine\u001B[0;34m(url, **kwargs)\u001B[0m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sqlalchemy/util/deprecations.py:281\u001B[0m, in \u001B[0;36mdeprecated_params.<locals>.decorate.<locals>.warned\u001B[0;34m(fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    274\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m kwargs:\n\u001B[1;32m    275\u001B[0m         _warn_with_version(\n\u001B[1;32m    276\u001B[0m             messages[m],\n\u001B[1;32m    277\u001B[0m             versions[m],\n\u001B[1;32m    278\u001B[0m             version_warnings[m],\n\u001B[1;32m    279\u001B[0m             stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[1;32m    280\u001B[0m         )\n\u001B[0;32m--> 281\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sqlalchemy/engine/create.py:602\u001B[0m, in \u001B[0;36mcreate_engine\u001B[0;34m(url, **kwargs)\u001B[0m\n\u001B[1;32m    600\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m kwargs:\n\u001B[1;32m    601\u001B[0m             dbapi_args[k] \u001B[38;5;241m=\u001B[39m pop_kwarg(k)\n\u001B[0;32m--> 602\u001B[0m     dbapi \u001B[38;5;241m=\u001B[39m \u001B[43mdbapi_meth\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdbapi_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    604\u001B[0m dialect_args[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbapi\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m dbapi\n\u001B[1;32m    606\u001B[0m dialect_args\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompiler_linting\u001B[39m\u001B[38;5;124m\"\u001B[39m, compiler\u001B[38;5;241m.\u001B[39mNO_LINTING)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py:696\u001B[0m, in \u001B[0;36mPGDialect_psycopg2.import_dbapi\u001B[0;34m(cls)\u001B[0m\n\u001B[1;32m    694\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    695\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mimport_dbapi\u001B[39m(\u001B[38;5;28mcls\u001B[39m):\n\u001B[0;32m--> 696\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpsycopg2\u001B[39;00m\n\u001B[1;32m    698\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m psycopg2\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'psycopg2'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Table Definitions ( SQLAlchemy ORM)\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class UserType(Base):\n",
    "    __tablename__ = 'usertype'\n",
    "    userTypeId = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    userType = Column(String(50), nullable=False)\n",
    "    description = Column(String(250))\n",
    "\n",
    "\n",
    "class Station(Base):\n",
    "    __tablename__ = 'station'\n",
    "    id = Column(String(50), primary_key=True)\n",
    "    name = Column(String(50), nullable=False)\n",
    "    oldName = Column(String(50), nullable=True)\n",
    "\n",
    "\n",
    "class Date(Base):\n",
    "    __tablename__ = 'date'\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    tripId = Column(String(50), nullable=False)\n",
    "    hour = Column(Integer, nullable=False)\n",
    "    day = Column(Integer, nullable=False)\n",
    "    month = Column(Integer, nullable=False)\n",
    "    year = Column(Integer, nullable=False)\n",
    "    minute = Column(Integer, nullable=False)\n",
    "    dateTime = Column(TIMESTAMP, nullable=False)\n",
    "\n",
    "\n",
    "class FactTrip(Base):\n",
    "    __tablename__ = 'fact_trip'\n",
    "    tripId = Column(String(50), primary_key=True)\n",
    "    startStation = Column(String(50), ForeignKey('station.id'), nullable=False)\n",
    "    stopStation = Column(String(50), ForeignKey('station.id'), nullable=False)\n",
    "    stopTime = Column(TIMESTAMP, nullable=False)\n",
    "    userTypeId = Column(Integer, ForeignKey('usertype.userTypeId'), nullable=False)\n",
    "    startTime = Column(TIMESTAMP, nullable=False)\n",
    "    duration = Column(Interval)\n",
    "    dateId = Column(Integer, ForeignKey('date.id'), nullable=False)\n",
    "\n",
    "\n",
    "Base.metadata.create_all(engine)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Load CSV Files (Extraction Phase)\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_december = pd.read_csv('CitiBike_December.csv')\n",
    "df_january = pd.read_csv('CitiBike_January.csv')\n",
    "df_february = pd.read_csv('CitiBike_February.csv')\n",
    "df_202501 = pd.read_csv('202501.csv')\n",
    "df_202502 = pd.read_csv('202502.csv')\n",
    "\n",
    "# Combines DataFrames\n",
    "df_all = pd.concat([df_december, df_january, df_february, df_202501, df_202502], ignore_index=True)\n",
    "\n",
    "print(f\"Combined data shape: {df_all.shape}\")\n",
    "df_all.head()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Cleansing & Transformation\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1:  duplicates\n",
    "df_all.drop_duplicates(inplace=True)\n",
    "\n",
    "# 2 : null values\n",
    "df_all.dropna(subset=['ride_id', 'started_at', 'ended_at', 'start_station_id', 'end_station_id', 'member_casual'],\n",
    "              inplace=True)\n",
    "\n",
    "# 3:  data types\n",
    "df_all['started_at'] = pd.to_datetime(df_all['started_at'])\n",
    "df_all['ended_at'] = pd.to_datetime(df_all['ended_at'])\n",
    "\n",
    "# 4:  duration\n",
    "df_all['duration'] = df_all['ended_at'] - df_all['started_at']\n",
    "\n",
    "# 5:  date dimension\n",
    "df_all['hour'] = df_all['started_at'].dt.hour\n",
    "df_all['day'] = df_all['started_at'].dt.day\n",
    "df_all['month'] = df_all['started_at'].dt.month\n",
    "df_all['year'] = df_all['started_at'].dt.year\n",
    "df_all['minute'] = df_all['started_at'].dt.minute\n",
    "\n",
    "# 6:  station dimension\n",
    "date_df = df_all[['ride_id', 'hour', 'day', 'month', 'year', 'minute', 'started_at']].copy()\n",
    "date_df.rename(columns={'started_at': 'dateTime', 'ride_id': 'tripId'}, inplace=True)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load UserType Dimension\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "usertype_map = {\n",
    "    'member': ('Member', 'Registered user'),\n",
    "    'casual': ('Casual', 'Unregistered user')\n",
    "}\n",
    "\n",
    "user_types = pd.DataFrame(usertype_map).T.reset_index()\n",
    "user_types.columns = ['member_casual', 'userType', 'description']\n",
    "\n",
    "# Load\n",
    "for _, row in user_types.iterrows():\n",
    "    user_type_entry = UserType(userType=row['userType'], description=row['description'])\n",
    "    session.merge(user_type_entry)\n",
    "session.commit()\n",
    "\n",
    "user_type_lookup = session.query(UserType).all()\n",
    "user_type_dict = {ut.userType.lower(): ut.userTypeId for ut in user_type_lookup}\n",
    "df_all['userTypeId'] = df_all['member_casual'].map(lambda x: user_type_dict.get(usertype_map[x][0].lower()))\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Station Dimension\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#  unique stations\n",
    "start_stations = df_all[['start_station_id', 'start_station_name']].drop_duplicates()\n",
    "start_stations.columns = ['id', 'name']\n",
    "stop_stations = df_all[['end_station_id', 'end_station_name']].drop_duplicates()\n",
    "stop_stations.columns = ['id', 'name']\n",
    "\n",
    "stations = pd.concat([start_stations, stop_stations]).drop_duplicates(subset='id')\n",
    "\n",
    "for _, row in stations.iterrows():\n",
    "    station_entry = Station(id=str(row['id']), name=row['name'])\n",
    "    session.merge(station_entry)\n",
    "session.commit()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Date Dimension"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "date_id_map = {}\n",
    "for _, row in date_df.iterrows():\n",
    "    date_entry = Date(\n",
    "        tripId=row['tripId'],\n",
    "        hour=row['hour'],\n",
    "        day=row['day'],\n",
    "        month=row['month'],\n",
    "        year=row['year'],\n",
    "        minute=row['minute'],\n",
    "        dateTime=row['dateTime']\n",
    "    )\n",
    "    session.add(date_entry)\n",
    "    session.flush()\n",
    "    date_id_map[row['tripId']] = date_entry.id\n",
    "session.commit()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Fact Table (FactTrip)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_all['dateId'] = df_all['ride_id'].map(date_id_map)\n",
    "\n",
    "for _, row in df_all.iterrows():\n",
    "    trip_entry = FactTrip(\n",
    "        tripId=row['ride_id'],\n",
    "        startStation=str(row['start_station_id']),\n",
    "        stopStation=str(row['end_station_id']),\n",
    "        stopTime=row['ended_at'],\n",
    "        userTypeId=row['userTypeId'],\n",
    "        startTime=row['started_at'],\n",
    "        duration=row['duration'],\n",
    "        dateId=row['dateId']\n",
    "    )\n",
    "    session.merge(trip_entry)\n",
    "session.commit()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Notes:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
